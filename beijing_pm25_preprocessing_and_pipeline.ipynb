{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6d372-83ab-4baf-8c05-2eef562ca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all the necessary dependencies\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91451908-b338-4585-87e2-af4fa852260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) LOAD & BASIC CLEAN ----------\n",
    "csv_path = \"PRSA_data_2010.1.1-2014.12.31.csv\" \n",
    "assert os.path.exists(csv_path), f\"CSV file not found at {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Original columns:\", list(df.columns)[:20])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ed23f-56bb-45d7-bed2-cc53dc50695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize column names to friendly lowercase identifiers\n",
    "def clean_col(c):\n",
    "    c = str(c).strip().lower()\n",
    "    # unify pm2.5 -> pm25\n",
    "    c = c.replace(\"pm2.5\", \"pm25\")\n",
    "    # replace non-alphanum with underscore\n",
    "    c = re.sub(r'[^0-9a-z]+', '_', c)\n",
    "    c = c.strip('_')\n",
    "    return c\n",
    "\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "print(\"Normalized columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c957898-7349-461b-a953-940ba62c7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) BUILD DATETIME ----------\n",
    "# UCI has year, month, day, hour columns; construct a single datetime column\n",
    "if set(['year', 'month', 'day', 'hour']).issubset(df.columns):\n",
    "    df['datetime'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    df.set_index('datetime', inplace=False)\n",
    "else:\n",
    "    # If dataset already has a timestamp column - try 'time' or 'date' detection\n",
    "    possible_ts = [c for c in df.columns if 'date' in c or 'time' in c or 'datetime' in c]\n",
    "    if possible_ts:\n",
    "        df['datetime'] = pd.to_datetime(df[possible_ts[0]])\n",
    "    else:\n",
    "        raise ValueError(\"Couldn't find (year,month,day,hour) nor timestamp column. Please add a datetime column.\")\n",
    "\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862b4f8-55db-4e20-bf2d-7d8d67a1de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) CYCLICAL ENCODING ----------\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb65114-c00f-45cf-ac00-03b78a9df46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) RENAME / INTERPRET COLUMNS ----------\n",
    "# Typical UCI fields: pm25, dewp, temp, pres, cbwd (wind direction categorical), iws (wind speed), is, ir\n",
    "# Map common names:\n",
    "col_map = {}\n",
    "if 'pm25' in df.columns:\n",
    "    col_map['pm25'] = 'pm25'\n",
    "if 'dewp' in df.columns:\n",
    "    col_map['dewp'] = 'dewpoint'\n",
    "if 'temp' in df.columns:\n",
    "    col_map['temp'] = 'temp'\n",
    "if 'pres' in df.columns:\n",
    "    col_map['pres'] = 'pressure'\n",
    "if 'iws' in df.columns:\n",
    "    col_map['iws'] = 'wind_speed'\n",
    "if 'cbwd' in df.columns:\n",
    "    col_map['cbwd'] = 'cbwd'   # categorical wind-dir\n",
    "    \n",
    "df = df.rename(columns=col_map)\n",
    "print(\"After rename, sample columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637312b-3d95-4ef1-8885-eac792a27ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) LAGS & ROLLING FEATURES ----------\n",
    "# We'll create lag features from pm25 (past values only)\n",
    "target_col = 'pm25'\n",
    "lags = [1, 3, 6, 12, 24]   # hours\n",
    "for lag in lags:\n",
    "    df[f'pm25_lag_{lag}'] = df[target_col].shift(lag)\n",
    "\n",
    "# rolling stats (use shifted series so we never use 'future' info)\n",
    "df['pm25_roll_mean_6h'] = df[target_col].shift(1).rolling(window=6, min_periods=1).mean()\n",
    "df['pm25_roll_std_6h']  = df[target_col].shift(1).rolling(window=6, min_periods=1).std().fillna(0)\n",
    "\n",
    "# slope / trend over last 3 hours (simple linear slope)\n",
    "def rolling_slope(series, window=3):\n",
    "    # compute slope using polyfit (fast enough)\n",
    "    arr = series.values\n",
    "    out = np.full_like(arr, np.nan, dtype=float)\n",
    "    for i in range(window-1, len(arr)):\n",
    "        x = np.arange(window)\n",
    "        y = arr[i-window+1:i+1]\n",
    "        if np.any(np.isnan(y)):\n",
    "            out[i] = np.nan\n",
    "        else:\n",
    "            out[i] = np.polyfit(x, y, 1)[0]\n",
    "    return pd.Series(out, index=series.index)\n",
    "\n",
    "df['pm25_slope_3h'] = rolling_slope(df[target_col].shift(1), window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a12dc3-75db-433f-b144-1cc69f6782c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6) WIND FEATURES ----------\n",
    "# UCI has cbwd categorical (NE, NW, SE, cv). We'll one-hot encode cbwd in the pipeline.\n",
    "# Optional: map cbwd -> approximate u/v (only if you want numeric wind vector)\n",
    "wind_map = {'NE':45, 'NW':315, 'SE':135, 'cv':np.nan}  # 'cv' often means \"calm / variable\"\n",
    "if 'cbwd' in df.columns:\n",
    "    df['cbwd'] = df['cbwd'].astype(str).replace('nan', np.nan)\n",
    "\n",
    "# if numeric wind_speed exists (iws), we can optionally estimate u/v by mapping cbwd to angle\n",
    "if 'wind_speed' in df.columns and 'cbwd' in df.columns:\n",
    "    ang = df['cbwd'].map(wind_map)\n",
    "    df['wind_u'] = df['wind_speed'] * np.cos(np.deg2rad(ang))\n",
    "    df['wind_v'] = df['wind_speed'] * np.sin(np.deg2rad(ang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69283f-14e4-4403-908e-0cc27c577712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 7) TRAFFIC & NEIGHBORS ----------\n",
    "# UCI does NOT include traffic_count or neighbor stations. If you have external traffic or other stations,\n",
    "# join them on datetime BEFORE running the pipeline.\n",
    "if 'traffic_count' not in df.columns:\n",
    "    print(\"Note: no 'traffic_count' column found. Traffic-based features will be skipped unless you merge traffic data.\")\n",
    "else:\n",
    "    df['traffic_roll3'] = df['traffic_count'].shift(1).rolling(3).mean()\n",
    "    df['traffic_roll6'] = df['traffic_count'].shift(1).rolling(6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee999d70-2026-43f5-bb57-7e7deeb5f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 8) MISSING TARGET HANDLING ----------\n",
    "# For supervised regression we need rows where current pm25 (target) is present.\n",
    "initial_len = len(df)\n",
    "df = df[~df[target_col].isna()].copy()   # drop rows where target is missing\n",
    "print(f\"Dropped {initial_len - len(df)} rows with missing target (pm25).\")\n",
    "\n",
    "# drop rows where any of the lag features are still NaN (start of series)\n",
    "lag_cols = [f'pm25_lag_{l}' for l in lags] + ['pm25_roll_mean_6h','pm25_roll_std_6h','pm25_slope_3h']\n",
    "df = df.dropna(subset=lag_cols, how='any').reset_index(drop=True)\n",
    "print(\"After dropping lag-NaN rows:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3ed6c-bba3-4d6d-a308-0b2ea3f7f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 9) DEFINE FEATURE LISTS ----------\n",
    "num_cols = []\n",
    "for c in ['dewpoint','temp','pressure','wind_speed','wind_u','wind_v']:\n",
    "    if c in df.columns:\n",
    "        num_cols.append(c)\n",
    "\n",
    "# add pm2.5 lags & rolling stats\n",
    "num_cols += [c for c in lag_cols if c in df.columns]\n",
    "\n",
    "# cyclical numeric encodings\n",
    "num_cols += ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "\n",
    "cat_cols = []\n",
    "for c in ['month','is_weekend','cbwd']:   # cbwd categorical wind-dir\n",
    "    if c in df.columns:\n",
    "        cat_cols.append(c)\n",
    "\n",
    "print(\"Numeric features (example):\", num_cols[:20])\n",
    "print(\"Categorical features (example):\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea906fc-11ee-4290-8e75-8ffb5f34df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM2.5 over time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df['datetime'], df['pm25'], alpha=0.7)\n",
    "plt.title(\"PM2.5 Concentration Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PM2.5 (µg/m³)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b815ecb-130f-4226-9e5c-516a5161258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly average trend\n",
    "df['month_name'] = df['datetime'].dt.to_period('M')\n",
    "monthly_mean = df.groupby('month_name')['pm25'].mean()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "monthly_mean.plot()\n",
    "plt.title(\"Monthly Average PM2.5\")\n",
    "plt.ylabel(\"PM2.5 (µg/m³)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd12cbb-0e60-4788-8c1b-0708a6c121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of PM2.5\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df['pm25'].dropna(), bins=50, alpha=0.7)\n",
    "plt.title(\"Distribution of PM2.5\")\n",
    "plt.xlabel(\"PM2.5 (µg/m³)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba75055-2ba2-4e82-835f-05faa0a23bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df[['pm25','temp','dewpoint','pressure','wind_speed']].corr(),\n",
    "            annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap (PM2.5 vs Weather)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6967bde-851f-4001-8dae-1f86a368aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag effect check (scatter of lag vs PM2.5)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df['pm25_lag_1'], df['pm25'], alpha=0.3)\n",
    "plt.title(\"Lag-1 PM2.5 vs Current PM2.5\")\n",
    "plt.xlabel(\"PM2.5 (t-1h)\")\n",
    "plt.ylabel(\"PM2.5 (t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b70dd-4ae4-4160-9e81-ade5180e843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 10) PREPROCESSOR (sklearn pipeline) ----------\n",
    "num_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95, svd_solver='full'))   # optional -- keeps 95% variance\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "], remainder='drop', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284fa75-dfba-4559-a1bc-b73419f9fc6d",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd6a59-feb4-4ac4-b9a7-488f2e20d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn, math\n",
    "from packaging import version\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cd989-6c8b-463b-86ed-942a559f7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533b3b7-6df9-484c-b9e4-34cf87bad5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn-version-safe OneHotEncoder argument\n",
    "ohe_kwargs = {}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kwargs['sparse_output'] = False\n",
    "else:\n",
    "    ohe_kwargs['sparse'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19da84-b68b-4882-b888-9132dab0393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a preprocessor WITHOUT PCA (keeps interpretability for now)\n",
    "num_pipe_no_pca = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', RobustScaler()),\n",
    "    # no PCA here for model interpretability\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', **ohe_kwargs))\n",
    "])\n",
    "\n",
    "preprocessor_no_pca = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipe_no_pca, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder='drop', verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ff8be-efe8-4588-8766-7578b967236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build selector + base models + stacking ensemble \n",
    "selector = SelectFromModel(RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42),\n",
    "                           threshold='median')\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(max_iter=300, learning_rate=0.05)\n",
    "rf = RandomForestRegressor(n_estimators=400, n_jobs=-1, random_state=42)\n",
    "et = ExtraTreesRegressor(n_estimators=400, n_jobs=-1, random_state=42)\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[('hgb', hgb), ('rf', rf), ('et', et)],\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_pipe = Pipeline([\n",
    "    ('pre', preprocessor_no_pca),\n",
    "    ('select', selector),\n",
    "    ('model', stack)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f107c-1923-465d-9fad-bcd59c60ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOLDOUT SPLIT: last 10% as time-based holdout\n",
    "n_total = len(df)\n",
    "holdout_frac = 0.10\n",
    "holdout_size = max(1, int(n_total * holdout_frac))\n",
    "train_df = df.iloc[:-holdout_size].reset_index(drop=True)\n",
    "hold_df  = df.iloc[-holdout_size:].reset_index(drop=True)\n",
    "\n",
    "X_train = train_df[num_cols + cat_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_hold  = hold_df[num_cols + cat_cols]\n",
    "y_hold  = hold_df[target_col]\n",
    "\n",
    "print(f\"Train rows: {len(X_train)}, Holdout rows: {len(X_hold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688359e6-774c-409f-bb78-aaf8d5512ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeries CV on training partition\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = cross_val_score(model_pipe, X_train, y_train,\n",
    "                            cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "print(\"CV MAE (train portion): {:.3f} ± {:.3f}\".format(-cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b542d7-13a1-4222-ba62-b4b5e248bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final model on full training partition\n",
    "print(\"Fitting final pipeline on full training partition (this may take time)...\")\n",
    "model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10dede-3852-450e-8c4a-25aa020d9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on holdout (true \"future\")\n",
    "y_hold_pred = model_pipe.predict(X_hold)\n",
    "mae_hold = mean_absolute_error(y_hold, y_hold_pred)\n",
    "rmse_hold = math.sqrt(mean_squared_error(y_hold, y_hold_pred))\n",
    "r2_hold = r2_score(y_hold, y_hold_pred)\n",
    "print(f\"Holdout MAE: {mae_hold:.3f}, RMSE: {rmse_hold:.3f}, R2: {r2_hold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c12e66-c229-4963-9839-829c6752046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
